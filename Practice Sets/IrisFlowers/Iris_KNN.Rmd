---
title: "k-Nearest Neighbors (KNN) example"
output: KNN_iris_notebook
---
```{r - Notebook Description}
## This notebook aims at describing how to apply a KNN technique (instance-based learning), where new data are classified based on stored, labeled instances. 

## Software: R-studio

## Tutorial can be found at https://www.datacamp.com/community/tutorials/machine-learning-in-r

```


```{r - Data import}
iris #already built-in dataset

# Print first lines
head(iris)

# Add column names
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")

# Check the result
iris

## Import libraries
library(ggvis)
library(class)
library(gmodels)
library(caret)
library(e1071)
```

```{r - Data Exploration}
# Division of `Species`
table(iris$Species) 

# Percentual division of `Species`
round(prop.table(table(iris$Species)) * 100, digits = 1)

# Summary overview of `iris`
summary(iris) 

# Refined summary overview
# summary(iris[c("Petal.Width", "Sepal.Width")])

## uses the ggvis package
iris %>% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %>% layer_points()
## evidence of correlations exist between sepal length and sepal width

iris %>% ggvis(~Petal.Length, ~Petal.Width, fill = ~Species) %>% layer_points()
## evidence of correlations exist between petal length and petal width
```

```{r - Hypothesis testing of correlations}
# Overall correlation `Petal.Length` and `Petal.Width`
cor(iris$Petal.Length, iris$Petal.Width)

# Return values of `iris` levels 
x=levels(iris$Species)

# Print Setosa correlation matrix
print(x[1])
cor(iris[iris$Species==x[1],1:4])

# Print Versicolor correlation matrix
print(x[2])
cor(iris[iris$Species==x[2],1:4])

# Print Virginica correlation matrix
print(x[3])
cor(iris[iris$Species==x[3],1:4])
```

```{r - Data preparation - Normalization}
## If you see that one attribute has a wide range of values, you will need to normalize your dataset, because this means that the distance will be dominated by this feature.

## the Iris data is self-consistent and does not need to be normalized but we will epxlore how to do it anyway

# Build your own `normalize()` function
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}

# Normalize the `iris` data
iris_norm <- as.data.frame(lapply(iris[1:4], normalize))

# Summarize `iris_norm`
summary(iris_norm)

```

```{r - Training and Test Sets}
## rule of thumb to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set.

## Need to make sure that all three classes of species are present in the training model with equal representation

set.seed(1234) #set see to ensure a consistent sequence of random numbers

## we now need an randomly generated index to pull an approximately equal representation of data points to compile our trianing dataset
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))

# Compose training set
iris.training <- iris[ind==1, 1:4]

# Inspect training set
head(iris.training)

# Compose test set
iris.test <- iris[ind==2, 1:4]

# Inspect test set
head(iris.test)

## Now that we have the raw data pre-processed, we need to obtain the attributes, Species in a separate vectors using the same indexing as above.

# Compose `iris` training labels
iris.trainLabels <- iris[ind==1,5]

# Inspect result
print(iris.trainLabels)

# Compose `iris` test labels
iris.testLabels <- iris[ind==2, 5]

# Inspect result
print(iris.testLabels)
```

```{r - Applying the KNN Model}
## Apply the KNN function to the training/test data, k=3 assists in the voting process to avoid ties

iris_pred <- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)

iris_pred
```

```{r - Model Evaluation}

# Put `iris.testLabels` in a data frame
irisTestLabels <- data.frame(iris.testLabels)

# Merge `iris_pred` and `iris.testLabels` 
merge <- data.frame(iris_pred, iris.testLabels)

# Specify column names for `merge`
names(merge) <- c("Predicted Species", "Observed Species")

# Inspect `merge` 
merge

## generate a cross tabulation (contingency table) to understand the relationship between two variables (in this case we want to compare how our test data related to our model results)
## Note that the last argument prop.chisq indicates whether or not the chi-square contribution of each cell is included. The chi-square statistic is the sum of the contributions from each of the individual cells and is used to decide whether the difference between the observed and the expected values is significant.

CrossTable(x = iris.testLabels, y = iris_pred, prop.chisq=FALSE)
```

```{r - An alternate approach using caret}

## Using caret functions can quickly partition the dataset into training/test data
# Create index to split based on labels  
index <- createDataPartition(iris$Species, p=0.75, list=FALSE)

# Subset training set with index
iris.training <- iris[index,]

# Subset test set with index
iris.test <- iris[-index,]

# Overview of algos supported by caret
names(getModelInfo())

# Train a model
model_knn <- train(iris.training[, 1:4], iris.training[, 5], method='knn')
```
```{r - Model Evaluation}

# Predict the labels of the test set
predictions<-predict(object=model_knn,iris.test[,1:4])

# Evaluate the predictions
table(predictions)

# Confusion matrix 
confusionMatrix(predictions,iris.test[,5])
```
```{r - Study the impacts of preprocess on model effectiveness}
# Train the model with preprocessing
model_knn <- train(iris.training[, 1:4], iris.training[, 5], method='knn', preProcess=c("center", "scale"))

# Predict values
predictions<-predict.train(object=model_knn,iris.test[,1:4], type="raw")

# Confusion matrix
confusionMatrix(predictions,iris.test[,5])
```

